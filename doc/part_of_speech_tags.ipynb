{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tags\n",
    "\n",
    "## References\n",
    "\n",
    "* [English Word Classes](https://web.stanford.edu/~jurafsky/slp3/8.pdf), Chapter 8.1, Speech and Language Processing\n",
    "* [Part-of-Speech Tagging](https://web.stanford.edu/~jurafsky/slp3/8.pdf), Chapter 8.2, Speech and Language Processing\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Part-of-Speech Tagset](#Part-of-Speech-Tagset)\n",
    "* [Download Data](#Download-Data)\n",
    "* [Read Data](#Read-Data)\n",
    "* [Predict Data](#Predict-Data)\n",
    "  * [Unigram Model](#Unigram-Model])\n",
    "  * [Bigram Model](#Bigram-Model])\n",
    "  * [NLTK Model](#NLTK-Model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagset\n",
    "\n",
    "A part-of-speech (POS) is a category to which a word is assigned in accordance with its syntactic functions.\n",
    "\n",
    "```\n",
    "John    Noun\n",
    "is      Verb\n",
    "a       Determiner\n",
    "boy     Noun\n",
    ".       Punctuation\n",
    "```\n",
    "\n",
    "The [Penn Treebank](https://www.aclweb.org/anthology/J93-2004/) project defined a fine-grained POS tagset, that was extended by the [OntoNotes](https://www.aclweb.org/anthology/W13-3516/) project:\n",
    "\n",
    "### Words\n",
    "\n",
    "| Tag | Description | Tag | Description |\n",
    "|:---|:---|:---|:---|\n",
    "| `ADD` | Email                                   | `POS` | Possessive ending |\n",
    "| `AFX` | Affix                                   | `PRP` | Personal pronoun |\n",
    "| `CC` | Coordinating conjunction                 | `PRP$` | Possessive pronoun  |\n",
    "| `CD` | Cardinal number                          | `RB` | Adverb |\n",
    "| `CODE` | Code ID                                | `RBR` | Adverb, comparative |\n",
    "| `DT` | Determiner                               | `RBS` | Adverb, superlative |\n",
    "| `EX` | Existential there                        | `RP` | Particle |\n",
    "| `FW` | Foreign word                             | `TO` | To |\n",
    "| `GW` | Go with                                  | `UH` | Interjection |\n",
    "| `IN` | Preposition or subordinating conjunction | `VB` | Verb, base form |\n",
    "| `JJ` | Adjective                                | `VBD` | Verb, past tense |\n",
    "| `JJR` | Adjective, comparative                  | `VBG` | Verb, gerund or present participle |\n",
    "| `JJS` | Adjective, superlative                  | `VBN` | Verb, past participle |\n",
    "| `LS` | List item marker                         | `VBP` | Verb, non-3rd person singular present |\n",
    "| `MD` | Modal                                    | `VBZ` | Verb, 3rd person singular present |\n",
    "| `NN` | Noun, singular or mass                   | `WDT` | *Wh*-determiner |\n",
    "| `NNS` | Noun, plural                            | `WP` | *Wh*-pronoun |\n",
    "| `NNP` | Proper noun, singular                   | `WP$` | *Wh*-pronoun, possessive |\n",
    "| `NNPS` | Proper noun, plural                    | `WRB` | *Wh*-adverb |\n",
    "| `PDT` | Predeterminer                           | `XX` | Unknown |\n",
    "\n",
    "### Symbols\n",
    "\n",
    "| Tag | Description | Tag | Description |\n",
    "|:---|:---|:---|:---|\n",
    "| `$` | Dollar | `-LRB-` | Left bracket |\n",
    "| `:` | Colon | `-RRB-` | Right bracket |\n",
    "| `,` | Comma | `HYPH` | Hyphen |\n",
    "| `.` | Period | `NFP` | Superfluous punctuation |\n",
    "| ` `` ` | Left quote | `SYM` | Symbol |\n",
    "| `''` | Right quote | `PUNC` | General punctuation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Retrieve the path to the `cs329` project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jdchoi/workspace/cs329/doc\n",
      "/Users/jdchoi/workspace/cs329 <class 'pathlib.PosixPath'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd()\n",
    "print(path)\n",
    "\n",
    "while path.name != 'cs329':\n",
    "    path = path.parent\n",
    "\n",
    "print(path, type(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [`pathlib`](https://docs.python.org/3/library/pathlib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `dat/pos` directory under the `cs329` project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jdchoi/workspace/cs329/dat/pos\n"
     ]
    }
   ],
   "source": [
    "path /= 'dat/pos'\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [`Path.mkdir()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [training set](https://raw.githubusercontent.com/emory-courses/cs329/master/dat/pos/wsj-pos.trn.gold.tsv) and the [development set](https://raw.githubusercontent.com/emory-courses/cs329/master/dat/pos/wsj-pos.dev.gold.tsv) for part-of-speech tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(remote_addr: str, local_addr: str):\n",
    "    r = requests.get(remote_addr)\n",
    "\n",
    "    with open(local_addr, 'wb') as fin:\n",
    "        fin.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [`requests`](https://requests.readthedocs.io/en/master/user/quickstart/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/emory-courses/cs329/master/dat/pos/wsj-pos.{}.gold.tsv'\n",
    "\n",
    "remote = url.format('trn')\n",
    "download(remote, path / Path(remote).name)\n",
    "\n",
    "remote = url.format('dev')\n",
    "download(remote, path / Path(remote).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Retrieve the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename: str):\n",
    "    data, sentence = [], []\n",
    "    fin = open(filename)\n",
    "    \n",
    "    for line in fin:\n",
    "        l = line.split()\n",
    "        if l:\n",
    "            sentence.append((l[0], l[1]))\n",
    "        else:\n",
    "            data.append(sentence)\n",
    "            sentence = []\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38219\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "trn_data = read_data(path / 'wsj-pos.trn.gold.tsv')\n",
    "print(len(trn_data))\n",
    "print(trn_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `word_count()` that counts the number of words in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def word_count(data: List[List[Tuple[str, str]]]) -> int:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple list where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: the total number of words in the data\n",
    "    \"\"\"\n",
    "    return sum([len(sentence) for sentence in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912344\n"
     ]
    }
   ],
   "source": [
    "print(word_count(trn_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model\n",
    "\n",
    "\n",
    "Let us write a function `create_uni_pos_dict()` that reads data and returns a dictionary where the key is a word and the value is the list of possible POS tags with probabilities in descending order such that:\n",
    "\n",
    "$$\n",
    "P(p|w) = \\frac{Count(w,p)}{Count(w)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "def create_uni_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is a word and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for word, pos in sentence:\n",
    "            model.setdefault(word, Counter()).update([pos])\n",
    "\n",
    "    for word, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[word] = [(pos, count/total) for pos, count in ts]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'A': 2, 'C': 2, 'B': 1})\n",
      "Counter({'C': 3, 'A': 2, 'B': 1})\n",
      "[3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "c.update(['A', 'B', 'A', 'C', 'C'])\n",
    "print(c)\n",
    "c.update('C')\n",
    "print(c)\n",
    "l = [count for _, count in c.most_common()]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the unigram dictionary from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_pos_dict = create_uni_pos_dict(trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 0.9714285714285714), ('VB', 0.01904761904761905), ('UH', 0.009523809523809525)]\n",
      "[('VB', 0.8293216630196937), ('VBP', 0.08971553610503283), ('NN', 0.06564551422319474), ('JJ', 0.015317286652078774)]\n"
     ]
    }
   ],
   "source": [
    "print(uni_pos_dict['man'])\n",
    "print(uni_pos_dict['buy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the unigram dictionary to predict POS tags of words in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_uni_pos_dict(uni_pos_dict: Dict[str, List[Tuple[str, float]]], tokens: List[str], pprint=False) -> List[Tuple[str, float]]:\n",
    "    def predict(token):\n",
    "        t = uni_pos_dict.get(token, None)\n",
    "        return t[0] if t else ('XX', 0.0)\n",
    "\n",
    "    output = [predict(token) for token in tokens]\n",
    "    if pprint:\n",
    "        for token, t in zip(tokens, output):\n",
    "            print('{:<15}{:<8}{:.2f}'.format(token, t[0], t[1]))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Input and Output](https://docs.python.org/3/tutorial/inputoutput.html)\n",
    "* [`zip()`](https://docs.python.org/3/library/functions.html#zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I              PRP     0.99\n",
      "bought         VBD     0.65\n",
      "a              DT      1.00\n",
      "car            NN      1.00\n",
      "yesterday      NN      0.98\n",
      "that           IN      0.60\n",
      "was            VBD     1.00\n",
      "blue           JJ      0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRP', 0.9915824915824916),\n",
       " ('VBD', 0.6474820143884892),\n",
       " ('DT', 0.9987005955603682),\n",
       " ('NN', 1.0),\n",
       " ('NN', 0.9813432835820896),\n",
       " ('IN', 0.6039103975139195),\n",
       " ('VBD', 1.0),\n",
       " ('JJ', 0.8571428571428571)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"I bought a car yesterday that was blue\".split()\n",
    "predict_uni_pos_dict(uni_pos_dict, tokens, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.            NNP     1.00\n",
      "Choi           XX      0.00\n",
      "has            VBZ     1.00\n",
      "a              DT      1.00\n",
      "good           JJ      0.96\n",
      "wifi           XX      0.00\n",
      "connection     NN      1.00\n",
      "from           IN      1.00\n",
      "Emory          XX      0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('NNP', 1.0),\n",
       " ('XX', 0.0),\n",
       " ('VBZ', 0.9993718592964824),\n",
       " ('DT', 0.9987005955603682),\n",
       " ('JJ', 0.9585798816568047),\n",
       " ('XX', 0.0),\n",
       " ('NN', 1.0),\n",
       " ('IN', 1.0),\n",
       " ('XX', 0.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"Dr. Choi has a good wifi connection from Emory\".split()\n",
    "predict_uni_pos_dict(uni_pos_dict, tokens, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the function `evaluate_uni_pos()` that estimates the accuracy of the unigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_uni_pos(uni_pos_dict: Dict[str, List[Tuple[str, float]]], data: List[List[Tuple[str, str]]]):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [t[0] for t in predict_uni_pos_dict(uni_pos_dict, tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "    print('{:5.2f}% ({}/{})'.format(100.0 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.88% (119754/131768)\n"
     ]
    }
   ],
   "source": [
    "dev_data = read_data(path / 'wsj-pos.dev.gold.tsv')\n",
    "evaluate_uni_pos(uni_pos_dict, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Write a function `create_bi_pos_dict()` that reads data and returns a dictionary where the key is the previous POS tag and the value is the list of possible POS tags with probabilities in descending order such that:\n",
    "\n",
    "$$\n",
    "P(p_i|p_{i-1}) = \\frac{Count(p_{i-1}, p_i)}{Count(p_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "PREV_DUMMY = '!@#$'\n",
    "\n",
    "def to_probs(model: Dict[Any, Counter]):\n",
    "    for feature, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[feature] = [(pos, count/total) for pos, count in ts]\n",
    "    return model\n",
    "\n",
    "def create_bi_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is the previous POS tag and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for i, (_, curr_pos) in enumerate(sentence):\n",
    "            prev_pos = sentence[i-1][1] if i > 0 else PREV_DUMMY\n",
    "            model.setdefault(prev_pos, Counter()).update([curr_pos])\n",
    "\n",
    "    return to_probs(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the bigram dictionary from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_pos_dict = create_bi_pos_dict(trn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use both the unigram and bigram dictionaries to predict POS tags of words in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bi_pos_dict(uni_pos_dict: Dict[str, List[Tuple[str, float]]], bi_pos_dict: Dict[str, List[Tuple[str, float]]], tokens: List[str]) -> List[Tuple[str, float]]:\n",
    "    output = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        pos = uni_pos_dict.get(tokens[i], None)\n",
    "        if pos is None:\n",
    "            pos = bi_pos_dict.get(output[i-1][0] if i > 0 else PREV_DUMMY, None)\n",
    "        output.append(pos[0] if pos else ('XX', 0.0))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the function `evaluate_bi_pos()` that estimates the accuracy of the bigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bi_pos(uni_pos_dict: Dict[str, List[Tuple[str, float]]], bi_pos_dict: Dict[str, List[Tuple[str, float]]], data: List[List[Tuple[str, str]]]):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [t[0] for t in predict_bi_pos_dict(uni_pos_dict, bi_pos_dict, tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "    print('{:5.2f}% ({}/{})'.format(100.0 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the function `evaluate_bi_pos()` that estimates the accuracy of the unigram + bigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bi_pos(uni_pos_dict: Dict[str, List[Tuple[str, float]]], bi_pos_dict: Dict[str, List[Tuple[str, float]]], data: List[List[Tuple[str, str]]]):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [t[0] for t in predict_bi_pos_dict(uni_pos_dict, bi_pos_dict, tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "    print('{:5.2f}% ({}/{})'.format(100.0 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.01% (121234/131768)\n"
     ]
    }
   ],
   "source": [
    "evaluate_bi_pos(uni_pos_dict, bi_pos_dict, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "Let us write the following functions:\n",
    "\n",
    "* `create_bi_wp_dict()` that estimates $P(p_i|w_{i-1})$.\n",
    "* `create_bi_wn_dict()` that estimates $P(p_i|w_{i+1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bi_wp_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is the previous word and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for i, (_, curr_pos) in enumerate(sentence):\n",
    "            prev_word = sentence[i-1][0] if i > 0 else PREV_DUMMY\n",
    "            model.setdefault(prev_word, Counter()).update([curr_pos])\n",
    "\n",
    "    return to_probs(model)\n",
    "\n",
    "\n",
    "def create_bi_wn_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is the previous word and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for i, (_, curr_pos) in enumerate(sentence):\n",
    "            next_word = sentence[i+1][0] if i+1 < len(sentence) else PREV_DUMMY\n",
    "            model.setdefault(next_word, Counter()).update([curr_pos])\n",
    "\n",
    "    return to_probs(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two binary dictionaries from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_wp_dict = create_bi_wp_dict(trn_data)\n",
    "bi_wn_dict = create_bi_wn_dict(trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_interporlation(\n",
    "        uni_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wp_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wn_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        uni_pos_weight: float,\n",
    "        bi_pos_weight: float,\n",
    "        bi_wp_weight: float,\n",
    "        bi_wn_weight: float,\n",
    "        tokens: List[str]) -> List[Tuple[str, float]]:\n",
    "    output = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        scores = dict()\n",
    "        curr_word = tokens[i]\n",
    "        prev_pos = output[i-1][0] if i > 0 else PREV_DUMMY\n",
    "        prev_word = tokens[i-1] if i > 0 else PREV_DUMMY\n",
    "        next_word = tokens[i+1] if i+1 < len(tokens) else PREV_DUMMY\n",
    "\n",
    "        for pos, prob in uni_pos_dict.get(curr_word, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * uni_pos_weight\n",
    "\n",
    "        for pos, prob in bi_pos_dict.get(prev_pos, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * bi_pos_weight\n",
    "\n",
    "        for pos, prob in bi_wp_dict.get(prev_word, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * bi_wp_weight\n",
    "\n",
    "        for pos, prob in bi_wn_dict.get(next_word, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * bi_wn_weight\n",
    "\n",
    "        o = max(scores.items(), key=lambda t: t[1]) if scores else ('XX', 0.0)\n",
    "        output.append(o)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_interpolation(\n",
    "        uni_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wp_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wn_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        uni_pos_weight: float,\n",
    "        bi_pos_weight: float,\n",
    "        bi_wp_weight: float,\n",
    "        bi_wn_weight: float,\n",
    "        data: List[List[Tuple[str, str]]],\n",
    "        pprint=False):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [t[0] for t in predict_interporlation(uni_pos_dict, bi_pos_dict, bi_wp_dict, bi_wn_dict, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight, tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "        \n",
    "    accuracy = 100.0 * correct / total\n",
    "    print('{:5.2f}% - uni_pos: {:3.1f}, bi_pos: {:3.1f}, bi_wp: {:3.1f}, bi_np: {:3.1f}'.format(accuracy, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.25% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 1.0, bi_np: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.25129014631777"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_pos_weight = 1.0\n",
    "bi_pos_weight = 1.0\n",
    "bi_wp_weight = 1.0\n",
    "bi_wn_weight = 1.0\n",
    "evaluate_interpolation(uni_pos_dict, bi_pos_dict, bi_wp_dict, bi_wn_dict, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight, dev_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.25% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 0.1, bi_np: 0.1\n",
      "67.53% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 0.1, bi_np: 0.5\n",
      "53.88% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 0.1, bi_np: 1.0\n",
      "67.32% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 0.5, bi_np: 0.1\n",
      "65.99% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 0.5, bi_np: 0.5\n",
      "58.57% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 0.5, bi_np: 1.0\n",
      "54.19% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 1.0, bi_np: 0.1\n",
      "59.34% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 1.0, bi_np: 0.5\n",
      "58.84% - uni_pos: 0.1, bi_pos: 0.1, bi_wp: 1.0, bi_np: 1.0\n",
      "53.94% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 0.1, bi_np: 0.1\n",
      "62.74% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 0.1, bi_np: 0.5\n",
      "55.54% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 0.1, bi_np: 1.0\n",
      "53.08% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.1\n",
      "61.05% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.5\n",
      "58.78% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 0.5, bi_np: 1.0\n",
      "47.89% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 1.0, bi_np: 0.1\n",
      "56.09% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 1.0, bi_np: 0.5\n",
      "58.17% - uni_pos: 0.1, bi_pos: 0.5, bi_wp: 1.0, bi_np: 1.0\n",
      "20.11% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 0.1, bi_np: 0.1\n",
      "45.69% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 0.1, bi_np: 0.5\n",
      "53.83% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 0.1, bi_np: 1.0\n",
      "36.86% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 0.5, bi_np: 0.1\n",
      "50.34% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 0.5, bi_np: 0.5\n",
      "55.80% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 0.5, bi_np: 1.0\n",
      "41.53% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 1.0, bi_np: 0.1\n",
      "50.60% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 1.0, bi_np: 0.5\n",
      "55.44% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 1.0, bi_np: 1.0\n",
      "92.97% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 0.1, bi_np: 0.1\n",
      "92.68% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 0.1, bi_np: 0.5\n",
      "87.10% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 0.1, bi_np: 1.0\n",
      "92.85% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 0.5, bi_np: 0.1\n",
      "92.75% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 0.5, bi_np: 0.5\n",
      "87.81% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 0.5, bi_np: 1.0\n",
      "87.91% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 1.0, bi_np: 0.1\n",
      "88.03% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 1.0, bi_np: 0.5\n",
      "84.47% - uni_pos: 0.5, bi_pos: 0.1, bi_wp: 1.0, bi_np: 1.0\n",
      "93.10% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 0.1, bi_np: 0.1\n",
      "92.87% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 0.1, bi_np: 0.5\n",
      "87.29% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 0.1, bi_np: 1.0\n",
      "91.55% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.1\n",
      "91.25% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.5\n",
      "86.44% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 0.5, bi_np: 1.0\n",
      "84.72% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 1.0, bi_np: 0.1\n",
      "85.44% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 1.0, bi_np: 0.5\n",
      "82.41% - uni_pos: 0.5, bi_pos: 0.5, bi_wp: 1.0, bi_np: 1.0\n",
      "91.14% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 0.1, bi_np: 0.1\n",
      "90.42% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 0.1, bi_np: 0.5\n",
      "85.11% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 0.1, bi_np: 1.0\n",
      "87.04% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 0.5, bi_np: 0.1\n",
      "87.23% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 0.5, bi_np: 0.5\n",
      "83.16% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 0.5, bi_np: 1.0\n",
      "79.06% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 1.0, bi_np: 0.1\n",
      "80.58% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 1.0, bi_np: 0.5\n",
      "78.92% - uni_pos: 0.5, bi_pos: 1.0, bi_wp: 1.0, bi_np: 1.0\n",
      "92.73% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 0.1, bi_np: 0.1\n",
      "92.86% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 0.1, bi_np: 0.5\n",
      "92.28% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 0.1, bi_np: 1.0\n",
      "93.05% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 0.5, bi_np: 0.1\n",
      "93.46% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 0.5, bi_np: 0.5\n",
      "93.01% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 0.5, bi_np: 1.0\n",
      "92.81% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 1.0, bi_np: 0.1\n",
      "93.13% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 1.0, bi_np: 0.5\n",
      "92.80% - uni_pos: 1.0, bi_pos: 0.1, bi_wp: 1.0, bi_np: 1.0\n",
      "92.82% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 0.1, bi_np: 0.1\n",
      "93.24% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 0.1, bi_np: 0.5\n",
      "92.74% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 0.1, bi_np: 1.0\n",
      "93.17% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.1\n",
      "93.51% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.5\n",
      "93.12% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 0.5, bi_np: 1.0\n",
      "92.39% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 1.0, bi_np: 0.1\n",
      "92.82% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 1.0, bi_np: 0.5\n",
      "92.34% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 1.0, bi_np: 1.0\n",
      "92.96% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 0.1, bi_np: 0.1\n",
      "93.30% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 0.1, bi_np: 0.5\n",
      "92.81% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 0.1, bi_np: 1.0\n",
      "93.03% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 0.5, bi_np: 0.1\n",
      "93.28% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 0.5, bi_np: 0.5\n",
      "92.73% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 0.5, bi_np: 1.0\n",
      "91.37% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 1.0, bi_np: 0.1\n",
      "91.83% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 1.0, bi_np: 0.5\n",
      "91.25% - uni_pos: 1.0, bi_pos: 1.0, bi_wp: 1.0, bi_np: 1.0\n",
      "==========================================================\n",
      "Best : 93.51% - uni_pos: 1.0, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.5\n",
      "Worst: 20.11% - uni_pos: 0.1, bi_pos: 1.0, bi_wp: 0.1, bi_np: 0.1\n"
     ]
    }
   ],
   "source": [
    "grid = [0.1, 0.5, 1.0]\n",
    "best = (0, None)\n",
    "worst = (100, None)\n",
    "\n",
    "for uni_pos_weight in grid:\n",
    "    for bi_pos_weight in grid:\n",
    "        for bi_wp_weight in grid:\n",
    "            for bi_wn_weight in grid:\n",
    "                acc = evaluate_interpolation(uni_pos_dict, bi_pos_dict, bi_wp_dict, bi_wn_dict, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight, dev_data)\n",
    "                if acc > best[0]: best = (acc, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight)\n",
    "                if acc < worst[0]: worst = (acc, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight)\n",
    "\n",
    "print('==========================================================')\n",
    "print('Best : {:5.2f}% - uni_pos: {:3.1f}, bi_pos: {:3.1f}, bi_wp: {:3.1f}, bi_np: {:3.1f}'.format(*best))\n",
    "print('Worst: {:5.2f}% - uni_pos: {:3.1f}, bi_pos: {:3.1f}, bi_wp: {:3.1f}, bi_np: {:3.1f}'.format(*worst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Model\n",
    "\n",
    "NLTK provides a POS tagger that takes a list of tokens and predicts the POS tags of those tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jdchoi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jdchoi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bought', 'a', 'car', 'yesterday', 'that', 'was', 'blue', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"I bought a car yesterday that was blue.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('bought', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('car', 'NN'),\n",
       " ('yesterday', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('was', 'VBD'),\n",
       " ('blue', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the function `evaluate_nltk_pos()` that estimates the accuracy of the NLTK model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nltk(data: List[List[Tuple[str, str]]]):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [pos for token, pos in nltk.pos_tag(tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "    print('{:5.2f}% ({}/{})'.format(100.0 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.14% (126685/131768)\n"
     ]
    }
   ],
   "source": [
    "evaluate_nltk(dev_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
