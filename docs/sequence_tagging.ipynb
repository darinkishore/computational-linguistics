{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Retrieve the path to the `cs329` project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jdchoi/workspace/cs329\n",
      "<class 'pathlib.PosixPath'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd()\n",
    "\n",
    "while path.name != 'cs329':\n",
    "    path = path.parent\n",
    "\n",
    "print(path)\n",
    "print(type(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `dat/pos` directory under the `cs329` project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jdchoi/workspace/cs329/dat/pos\n"
     ]
    }
   ],
   "source": [
    "path /= 'dat/pos'\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [training set](https://raw.githubusercontent.com/emory-courses/cs329/master/dat/pos/wsj-pos.trn.gold.tsv) and the [development set](https://raw.githubusercontent.com/emory-courses/cs329/master/dat/pos/wsj-pos.dev.gold.tsv) for part-of-speech tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(remote_addr: str, local_addr: str):\n",
    "    r = requests.get(remote_addr)\n",
    "\n",
    "    with open(local_addr, 'wb') as fin:\n",
    "        fin.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/emory-courses/cs329/master/dat/pos/wsj-pos.{}.gold.tsv'\n",
    "\n",
    "remote = url.format('trn')\n",
    "download(remote, path / Path(remote).name)\n",
    "\n",
    "remote = url.format('dev')\n",
    "download(remote, path / Path(remote).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Retrieve the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pos(filename: str):\n",
    "    data, sentence = [], []\n",
    "    fin = open(filename)\n",
    "    \n",
    "    for line in fin:\n",
    "        l = line.split()\n",
    "        if l:\n",
    "            sentence.append((l[0], l[1]))\n",
    "        else:\n",
    "            data.append(sentence)\n",
    "            sentence = []\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38219\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "trn_data = read_pos(path / 'wsj-pos.trn.gold.tsv')\n",
    "print(len(trn_data))\n",
    "print(trn_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "How many tokens are in `trn_data`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912344\n",
      "912344\n"
     ]
    }
   ],
   "source": [
    "n = sum([len(sentence) for sentence in trn_data])\n",
    "print(n)\n",
    "\n",
    "n = 0\n",
    "for sentence in trn_data:\n",
    "    n += len(sentence)\n",
    "\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Map\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Given `trn_data`, create a dictionary whose key is POS tag and value is a unique ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EX', 'RP', '#', 'NN', ',', 'JJR', 'PDT', '-LRB-', 'VBP', 'LS', 'SYM', 'NNPS', 'VBN', 'WDT', 'PRP$', 'JJ', ':', 'VBG', 'WP', 'VBZ', 'RB', 'DT', '-RRB-', '``', 'RBR', 'IN', 'VBD', 'POS', '$', 'UH', 'FW', 'PRP', \"''\", 'TO', 'RBS', 'JJS', 'CD', '.', 'VB', 'NNP', 'MD', 'WRB', 'CC', 'WP$', 'NNS'}\n",
      "{'#': 0, '$': 1, \"''\": 2, ',': 3, '-LRB-': 4, '-RRB-': 5, '.': 6, ':': 7, 'CC': 8, 'CD': 9, 'DT': 10, 'EX': 11, 'FW': 12, 'IN': 13, 'JJ': 14, 'JJR': 15, 'JJS': 16, 'LS': 17, 'MD': 18, 'NN': 19, 'NNP': 20, 'NNPS': 21, 'NNS': 22, 'PDT': 23, 'POS': 24, 'PRP': 25, 'PRP$': 26, 'RB': 27, 'RBR': 28, 'RBS': 29, 'RP': 30, 'SYM': 31, 'TO': 32, 'UH': 33, 'VB': 34, 'VBD': 35, 'VBG': 36, 'VBN': 37, 'VBP': 38, 'VBZ': 39, 'WDT': 40, 'WP': 41, 'WP$': 42, 'WRB': 43, '``': 44}\n"
     ]
    }
   ],
   "source": [
    "tagset = {pos for sentence in trn_data for token, pos in sentence}\n",
    "print(tagset)\n",
    "\n",
    "label_map = {pos:i for i, pos in enumerate(sorted(list(tagset)))}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Given `trn_data`, create a dictionary whose key is a feature and value is a unique ID.\n",
    "The following features need to be extracted for each token $w_i$:\n",
    "\n",
    "* $w_i$: the word form of the current token\n",
    "* $w_{i-1}$: the word from of the previous token\n",
    "* $w_{i+1}$: the word from of the next token\n",
    "* $p_{i-1}$: the (predicted) POS tag of the previous token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = {}\n",
    "\n",
    "for sentence in trn_data:\n",
    "    for i in range(len(sentence)):\n",
    "        c = sentence[i]\n",
    "        p = sentence[i-1] if i-1 >= 0 else None\n",
    "        n = sentence[i+1] if i+1 < len(sentence) else None\n",
    "\n",
    "        fs = []\n",
    "        fs.append('f0' + c[0])\n",
    "        if p: fs.append('f1' + p[0])\n",
    "        if n: fs.append('f2' + n[0])\n",
    "        if p: fs.append('f3' + p[1])\n",
    "\n",
    "        for f in fs:\n",
    "            feature_map.setdefault(f, len(feature_map)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "3\n",
      "8\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sentence = trn_data[0]\n",
    "print(sentence)\n",
    "\n",
    "print(feature_map['f0'+'Vinken'])\n",
    "print(feature_map['f1'+'Vinken'])\n",
    "print(feature_map['f2'+'Vinken'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Instances\n",
    "\n",
    "Create a instance per token where the instnace is a tuple of the feature vector and the label ID for the corresponding token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def extract_instances(data: List[List[Tuple[str, str]]], label_map, feature_map, training=False):\n",
    "    instances = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            c = sentence[i]\n",
    "            p = sentence[i-1] if i-1 >= 0 else None\n",
    "            n = sentence[i+1] if i+1 < len(sentence) else None\n",
    "            \n",
    "            # label map\n",
    "            if training: y = label_map.setdefault(c[1], len(label_map))\n",
    "            else: y = label_map.get(c[1], -1)\n",
    "            if y < 0: continue\n",
    "            \n",
    "            fs = []\n",
    "            fs.append('f0' + c[0])\n",
    "            if p: fs.append('f1' + p[0])\n",
    "            if n: fs.append('f2' + n[0])\n",
    "            if p: fs.append('f3' + p[1])\n",
    "            \n",
    "            # feature map\n",
    "            if training: features = [feature_map.setdefault(f, len(feature_map)+1) for f in fs]\n",
    "            else: features = [feature_map[f] for f in fs if f in feature_map]\n",
    "            \n",
    "            features.append(0)\n",
    "            x = np.array(sorted(features))\n",
    "            instances.append((x, y))\n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "feature_map = {}\n",
    "\n",
    "trn_inst = extract_instances(trn_data, label_map, feature_map, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pierre', 'NNP') (array([0, 1, 2]), 0)\n",
      "('Vinken', 'NNP') (array([0, 3, 4, 5, 6]), 0)\n",
      "(',', ',') (array([0, 6, 7, 8, 9]), 1)\n",
      "('61', 'CD') (array([ 0, 10, 11, 12, 13]), 2)\n",
      "('years', 'NNS') (array([ 0, 14, 15, 16, 17]), 3)\n",
      "('old', 'JJ') (array([ 0,  5, 18, 19, 20]), 4)\n",
      "(',', ',') (array([ 0,  7, 21, 22, 23]), 1)\n",
      "('will', 'MD') (array([ 0, 11, 13, 24, 25]), 5)\n",
      "('join', 'VB') (array([ 0, 26, 27, 28, 29]), 6)\n",
      "('the', 'DT') (array([ 0, 30, 31, 32, 33]), 7)\n",
      "('board', 'NN') (array([ 0, 34, 35, 36, 37]), 8)\n",
      "('as', 'IN') (array([ 0, 38, 39, 40, 41]), 9)\n",
      "('a', 'DT') (array([ 0, 42, 43, 44, 45]), 7)\n",
      "('nonexecutive', 'JJ') (array([ 0, 37, 46, 47, 48]), 4)\n",
      "('director', 'NN') (array([ 0, 23, 49, 50, 51]), 8)\n",
      "('Nov.', 'NNP') (array([ 0, 41, 52, 53, 54]), 0)\n",
      "('29', 'CD') (array([ 0,  6, 55, 56, 57]), 2)\n",
      "('.', '.') (array([ 0, 17, 58, 59]), 10)\n"
     ]
    }
   ],
   "source": [
    "sentence = trn_data[0]\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    print(sentence[i], trn_inst[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
